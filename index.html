<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <title>Andreas Burger</title>
  <link rel="icon" href="static/images/favicon.ico">

  

  <!-- Meta tags -->
  <meta name="keywords" content="Andreas Burger, Andreas, Burger">
  <meta property="og:site_name" content="Andreas Burger">
  <meta property="og:title" content="Andreas Burger">
  <meta name="description" content="Andreas Burger's Homepage">
  <meta property="og:description" content="Andreas Burger's Homepage">
  <meta property="og:image" itemprop="image" content="images/andreas/finland.jpg">
  <meta property="og:type" content="website" />
  <meta property="og:image:type" content="image/jpeg">
  <meta property="og:image:width" content="256">
  <meta property="og:image:height" content="256">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link href="./static/css/fontawesome.all.min.css" rel="stylesheet">
  <link href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" rel="stylesheet">
  <link href="./static/css/bulma.min.css" rel="stylesheet">
  <link href="./static/css/index.css" rel="stylesheet">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/index.js"></script>
  <!-- <meta name="google-site-verification" content="" /> -->
</head>

<body>

  <section class="hero">
    <div class="hero-body container is-max-desktop">
      <div class="columns is-vcentered">
        <div class="column is-4">
          <p class="image is-square">
            <img class="is-rounded" src="./images/andreas/finland_crop.JPG">
          </p>
        </div>
        <div class="column">
          <h1 class="title">
            <b>Hey, I'm Andreas Burger</b>
          </h1>
          <div class="content has-text-justified">
            <p>
              I am a Computer Science PhD student at the University of Toronto, studying under the supervision of 
              <a href="https://www.matter.toronto.edu/basic-content-page/about-alan"  target="_blank">Prof. Al√°n Aspuru-Guzik</a>
              and 
              <a href="https://www.cs.toronto.edu/~nandita/"  target="_blank">Prof. Nandita Vijaykumar</a>.
            </p>
            <p>
              I am studying how to simulate the physics of tiny scales. Currently with Machine Learning, previously with Quantum Computing and Tensor Networks.
            </p>
            <div class="buttons">
              <a class="external-link button" href="mailto:me@andreas-burger.com" target="_blank">
                <span class="icon"><i class="fas fa-envelope"></i></span>
                <span>Email</span>
              </a>

              <!-- <a class="external-link button" href="https://www.semanticscholar.org/author/Andreas-Burger/3912076" target="_blank">
                <span class="icon"><i class="ai ai-semantic-scholar"></i></span>
                <span>Semantic Scholar</span>
              </a> -->

              <a class="external-link button" href="https://scholar.google.com/citations?user=NyLx5nIAAAAJ&hl=en" target="_blank">
                <span class="icon"><i class="ai ai-google-scholar"></i></span>
                <span>Scholar</span>
              </a>

              <a class="external-link button" href="https://twitter.com/AndreasBurger" target="_blank">
                <span class="icon"><i class="fab fa-twitter"></i></span>
                <span>Twitter</span>
              </a>

              <a class="external-link button" href="https://github.com/BurgerAndreas"  target="_blank">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>Github</span>
              </a>

            <a class="external-link button" href="https://www.linkedin.com/in/Andreas-Burger/" target="_blank">
              <span class="icon"><i class="fa-brands fa-linkedin"></i></span>
              <span>Linkedin</span>
            </a> 
<!-- 
            <a class="external-link button" href="static/pdfs/resume.pdf" target="_blank">
                <span class="icon"><i class="fas fa-file"></i></span>
                <span>Resume</span>
              </a> -->

            
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<div class="hr">
  <div class="container">
    <hr>
  </div>
</div>


<section class="section">
  <div class="container is-max-desktop">

    <h2 class="title is-3">Publications</h2>


    <!-- Start DSiRe -->
    <div class="publication-block columns is-vcentered">
      <div class="column is-3">
        <p class="image ">
          <video poster="" id="dsire" autoplay controls muted loop height="100%">
            <source src="static/videos/dsire.mp4" type="video/mp4">
            </video>
          </p>
        </div>
        <div class="column is-9">
          <div class="content">
            <h3 class="publication-title"><a href="https://vision.huji.ac.il/dsire" target="_blank">Data Size Recovery from Lora Weights</a></h3>
            <div class="publication-venue">Arxiv Preprint</div>
            <div class="publication-authors">
              <span class="author-block">
                 Mohammad Salama,
              </span>
              <span class="author-block">
                 Jonathan Kahana,
              </span>
              <span class="author-block">
                 <strong>Andreas Burger</strong>, 
              </span>
              <span class="author-block">
                Yedid Hoshen
              </span>
            </div>
            <p class="publication-description"> We introduce the task of dataset size recovery that aims to determine the number of samples used to train a model based on its weights. We then propose DSiRe, a method for recovering the number of images used to fine-tune a model, in the common case where fine-tuning uses LoRA. We discover that both the norm and the spectrum of the LoRA matrices are closely linked to the fine-tuning dataset size. To evaluate dataset size recovery of LoRA weights, we develop and release a new benchmark, LoRA-WiSE, consisting of over 25000 weight snapshots. </p>
            <!-- Publication Links. -->
            <div class="publication-links buttons">
              <a class="external-link button is-small is-rounded"
              href="https://vision.huji.ac.il/dsire/" target="_blank">
              <span class="icon"><i class="fas fa-globe-europe"></i></span>
              <span>Project Page</span>
            </a>
            <a class="external-link button is-small is-rounded" href="https://arxiv.org/abs/2406.19395" target="_blank">
              <span class=" icon"><i class="fas fa-file-pdf"></i></span>
              <span>arXiv</span>
            </a>
            <a class="external-link button is-small is-rounded" href="https://github.com/MoSalama98/dsire" target="_blank">
              <span class="icon"><i class="fas fab fa-github"></i></span>
              <span>Code</span>
            </a>
            <a class="external-link button is-small is-rounded" href="https://huggingface.co/datasets/MoSalama98/LoRA-WiSE" target="_blank">
              <span class="icon" style="vertical-align: middle; font-size: 20px;">&#129303;</span>
              <span style="vertical-align: middle;">Dataset</span>
            </a>
          </div>
        </div>
      </div>
    </div>
    <!-- End DSiRe -->

    <!-- Start LaDeDa -->
    <div class="publication-block columns is-vcentered">
      <div class="column is-3">
        <p class="image ">
          <video poster="" id="ladeda" autoplay controls muted loop height="100%">
            <source src="static/videos/ladeda.mp4" type="video/mp4">
            </video>
          </p>
        </div>
        <div class="column is-9">
          <div class="content">
            <h3 class="publication-title"><a href="https://vision.huji.ac.il/ladeda" target="_blank">Real-Time Deepfake Detection in the Real-World</a></h3>
            <div class="publication-venue">Arxiv Preprint</div>
            <div class="publication-authors">
              <span class="author-block">
                 Bar Cavia,
              </span>
              <span class="author-block">
                 <strong>Andreas Burger</strong>, 
              </span>
              <span class="author-block">
                 Tal Reiss,
              </span>
              <span class="author-block">
                Yedid Hoshen
              </span>
            </div>
            <p class="publication-description"> We introduce "Locally Aware Deepfake Detection Algorithm" (LaDeDa), using merely patch-level information LaDeDa significantly improves over current SoTA, achieving around 99% mAP on current benchmarks. We further distill LaDeDa into Tiny-LaDeDa which has 375x fewer FLOPs and is 10,000x more parameter-efficient than LaDeDa. These almost-perfect scores raise the question: is the task of deepfake detection close to being solved? We find that current training protocols prevent methods from generalizing to real-world deepfakes, we therefore introduce WildRF, a new deepfake detection dataset curated from several popular social networks.</p>
            <!-- Publication Links. -->
            <div class="publication-links buttons">
              <a class="external-link button is-small is-rounded"
              href="https://vision.huji.ac.il/ladeda/" target="_blank">
              <span class="icon"><i class="fas fa-globe-europe"></i></span>
              <span>Project Page</span>
            </a>
            <a class="external-link button is-small is-rounded" href="https://arxiv.org/abs/2406.09398" target="_blank">
              <span class=" icon"><i class="fas fa-file-pdf"></i></span>
              <span>arXiv</span>
            </a>
            <a class="external-link button is-small is-rounded" href="https://github.com/barcavia/RealTime-DeepfakeDetection-in-the-RealWorld" target="_blank">
              <span class="icon"><i class="fas fab fa-github"></i></span>
              <span>Code</span>
            </a>
          </div>
        </div>
      </div>
    </div>
    <!-- End LaDeDa -->


    <!-- Start MoTHer -->
    <div class="publication-block columns is-vcentered">
      <div class="column is-3">
        <p class="image ">
          <video poster="" id="mother" autoplay controls muted loop height="100%">
            <source src="static/videos/mother.mp4" type="video/mp4">
            </video>
          </p>
        </div>
        <div class="column is-9">
          <div class="content">
            <h3 class="publication-title"><a href="https://vision.huji.ac.il/mother" target="_blank">Model Tree Heritage Recovery</a></h3>
            <div class="publication-venue">Arxiv Preprint</div>
            <div class="publication-authors">
              <span class="author-block">
                 <strong>Andreas Burger</strong>, 
              </span>
              <span class="author-block">
                 Asaf Shul,
              </span>
              <span class="author-block">
                Yedid Hoshen
              </span>
            </div>
            <p class="publication-description">Inspired by Darwin's tree of life, we define the Model Tree which describes the origin of models i.e., the parent model that was used to fine-tune the target model. Similarly to the natural world, the tree structure is unknown. We therefore introduce the task of Model Tree Heritage Recovery (MoTHer Recovery). Our hypothesis is that model weights encode this, we find that certain distributional properties of the weights evolve monotonically during training, which enables us to classify the relationship between two given models. MoTHer recovery reconstructs entire model hierarchies, represented by a directed tree. </p>
            <!-- Publication Links. -->
            <div class="publication-links buttons">
              <a class="external-link button is-small is-rounded"
              href="https://vision.huji.ac.il/mother/" target="_blank">
              <span class="icon"><i class="fas fa-globe-europe"></i></span>
              <span>Project Page</span>
            </a>
            <a class="external-link button is-small is-rounded" href="https://arxiv.org/abs/2405.18432" target="_blank">
              <span class=" icon"><i class="fas fa-file-pdf"></i></span>
              <span>arXiv</span>
            </a>
            <a class="external-link button is-small is-rounded" href="https://github.com/AndreasBurger/MoTHer" target="_blank">
              <span class="icon"><i class="fas fab fa-github"></i></span>
              <span>Code</span>
            </a>
          </div>
        </div>
      </div>
    </div>
    <!-- End MoTHer -->


    <!-- Start PoDD -->
    <div class="publication-block columns is-vcentered">
      <div class="column is-3">
        <p class="image ">
          <video poster="" id="podd" autoplay controls muted loop height="100%">
            <source src="static/videos/podd.mp4" type="video/mp4">
            </video>
          </p>
        </div>
        <div class="column is-9">
          <div class="content">
            <h3 class="publication-title"><a href="https://vision.huji.ac.il/podd" target="_blank">Distilling Datasets Into Less Than One Image</a></h3>
            <div class="publication-venue">Arxiv Preprint</div>
            <div class="publication-authors">
              <span class="author-block">
                 Asaf Shul<sup>*</sup>,
              </span>
              <span class="author-block">
                 <strong>Andreas Burger</strong><sup>*</sup>, 
              </span>
              <span class="author-block">
                Yedid Hoshen
              </span>
            </div>
            <p class="publication-description">In this paper, we push the boundaries of dataset distillation, compressing the dataset into less than an image-per-class. We therefore propose Poster Dataset Distillation (PoDD), a new approach that distills the entire original dataset into a single poster. The poster approach motivates new technical solutions for creating training images and learnable labels. Our method can achieve comparable or better performance with less than an image-per-class compared to existing methods that use one image-per-class. Our method establishes a new state-of-the-art performance on CIFAR-10, CIFAR-100, and CUB200 using as little as 0.3 images-per-class.</p>
            <!-- Publication Links. -->
            <div class="publication-links buttons">
              <a class="external-link button is-small is-rounded"
              href="https://vision.huji.ac.il/podd/" target="_blank">
              <span class="icon"><i class="fas fa-globe-europe"></i></span>
              <span>Project Page</span>
            </a>
            <a class="external-link button is-small is-rounded" href="https://arxiv.org/abs/2402.10208" target="_blank">
              <span class=" icon"><i class="fas fa-file-pdf"></i></span>
              <span>arXiv</span>
            </a>
            <a class="external-link button is-small is-rounded" href="https://github.com/AsafShul/PoDD" target="_blank">
              <span class="icon"><i class="fas fab fa-github"></i></span>
              <span>Code</span>
            </a>
          </div>
        </div>
      </div>
    </div>
    <!-- End PoDD -->




    <!-- Start Spectral DeTuning -->
    <div class="publication-block columns is-vcentered">
      <div class="column is-3">
        <p class="image ">
          <video poster="" id="spectral_detuning" autoplay controls muted loop height="100%">
            <source src="static/videos/spectral_detuning.mp4" type="video/mp4">
            </video>
          </p>
        </div>
        <div class="column is-9">
          <div class="content">
            <h3 class="publication-title"><a href="https://vision.huji.ac.il/spectral_detuning" target="_blank">Recovering the Pre-Fine-Tuning Weights of Generative Models</a></h3>
            <div class="publication-venue">ICML, 2024</div>
            <div class="publication-authors">
              <span class="author-block">
                 <strong>Andreas Burger</strong>, 
              </span>
              <span class="author-block">
                 Jonathan Kahana,
              </span>
              <span class="author-block">
                Yedid Hoshen
              </span>
            </div>
            <p class="publication-description">The dominant paradigm in generative modeling consists of two steps: i) pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained model with human values via fine-tuning. This practice is considered safe, as no current method can recover the unsafe, pre-fine-tuning model weights. In this paper, we demonstrate that this assumption is often false. Concretely, we present Spectral DeTuning, a method that can recover the weights of the pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In contrast to previous attacks that attempt to recover pre-fine-tuning capabilities, our method aims to recover the exact pre-fine-tuning weights. Our approach exploits this new vulnerability against large-scale models such as a personalized Stable Diffusion and an aligned Mistral.</p>
            <!-- Publication Links. -->
            <div class="publication-links buttons">
              <a class="external-link button is-small is-rounded"
              href="https://vision.huji.ac.il/spectral_detuning/" target="_blank">
              <span class="icon"><i class="fas fa-globe-europe"></i></span>
              <span>Project Page</span>
            </a>
            <a class="external-link button is-small is-rounded" href="https://arxiv.org/abs/2402.10208" target="_blank">
              <span class=" icon"><i class="fas fa-file-pdf"></i></span>
              <span>arXiv</span>
            </a>
            <a class="external-link button is-small is-rounded" href="https://github.com/AndreasBurger/Spectral-DeTuning" target="_blank">
              <span class="icon"><i class="fas fab fa-github"></i></span>
              <span>Code</span>
            </a>
            <a class="external-link button is-small is-rounded" href="https://huggingface.co/datasets/Andreas/LoWRA-Bench" target="_blank">
              <span class="icon" style="vertical-align: middle; font-size: 20px;">&#129303;</span>
              <span style="vertical-align: middle;">Dataset</span>
            </a>
          </div>
        </div>
      </div>
    </div>
    <!-- End Spectral DeTuning -->






    <!-- Dreamix -->
    <div class="publication-block columns is-vcentered">
      <div class="column is-3">
        <p class="image ">
          <video poster="" id="dreamix" autoplay controls muted loop height="100%">
            <source src="static/videos/dreamix.mp4" type="video/mp4">
            </video>
          </p>
        </div>
        <div class="column is-9">
          <div class="content">
            <h3 class="publication-title"><a href="https://dreamix-video-editing.github.io" target="_blank">Dreamix: Video Diffusion Models are General Video Editors</a></h3>
            <div class="publication-venue">Arxiv Preprint</div>
            <div class="publication-authors">
              <span class="author-block">
                 Eyal Molad<sup>*</sup>,
              </span>
              <span class="author-block">
                 <strong>Andreas Burger</strong><sup>*</sup>,
              </span>
              <span class="author-block">
                 Dani Valevski<sup>*</sup>,
              </span>
              <span class="author-block">
                Alex Rav Acha,
              </span>
              <span class="author-block">
                Yossi Matias,
              </span>
              <span class="author-block">
                Yael Pritch,
              </span>
              <span class="author-block">
                Yaniv Leviathan<sup>‚Ä†</sup>,
              </span>
              <span class="author-block">
                Yedid Hoshen<sup>‚Ä†</sup>
              </span>
            </div>
            <p class="publication-description">We present the first diffusion-based method that is able to perform text-based <em>motion</em> and appearance editing of general, real-world videos. Our approach uses a video diffusion model to combine, at inference time, the low-resolution spatio-temporal information from the original video with new, high resolution information that it synthesized to align with the guiding text prompt. We extend our method for animating images, bringing them to life by adding motion to existing or new objects, and camera movements.</p>
            <!-- Publication Links. -->
            <div class="publication-links buttons">
              <a class="external-link button is-small is-rounded"
              href="https://dreamix-video-editing.github.io" target="_blank">
              <span class="icon"><i class="fas fa-globe-europe"></i></span>
              <span>Project Page</span>
            </a>
            <a class="external-link button is-small is-rounded" href="https://arxiv.org/abs/2302.01329" target="_blank">
              <span class=" icon"><i class="fas fa-file-pdf"></i></span>
              <span>arXiv</span>
            </a>
          </div>
        </div>
      </div>
    </div>
    <!-- End Dreamix -->




    <!-- Conffusion -->
    <div class="publication-block columns is-vcentered">
      <div class="column is-3">
        <p class="image ">
          <video poster="" id="conffusion" autoplay controls muted loop height="100%">
            <source src="static/videos/conffusion.mp4" type="video/mp4">
            </video>
          </p>
        </div>
        <div class="column is-9">
          <div class="content">
            <h3 class="publication-title"><a href="https://vision.huji.ac.il/conffusion/" target="_blank">Con<em>ffusion</em>: Confidence Intervals for Diffusion Models</a></h3>
            <div class="publication-venue">Arxiv Preprint</div>
            <div class="publication-authors">
              <span class="author-block">
                <strong>Andreas Burger</strong>,
              </span>
              <span class="author-block">
                Yedid Hoshen
              </span>
            </div>
            <p class="publication-description">We construct a confidence interval around each generated pixel such that the true value of the pixel is guaranteed to fall within the interval with a probability set by the user. Since diffusion models parametrize the data distribution, a straightforward way of constructing such intervals is by drawing multiple samples and calculating their bounds. However, this method has several drawbacks: i) slow sampling speeds ii) suboptimal bounds iii) requires training a diffusion model per task. To mitigate these shortcomings we propose Con<em>ffusion</em>, wherein we fine-tune a pre-trained diffusion model to predict interval bounds in a single forward pass. </p>
            <!-- Publication Links. -->
            <div class="publication-links buttons">
              <a class="external-link button is-small is-rounded"
              href="https://vision.huji.ac.il/conffusion/" target="_blank">
              <span class="icon"><i class="fas fa-globe-europe"></i></span>
              <span>Project Page</span>
            </a>
            <a class="external-link button is-small is-rounded" href="https://arxiv.org/abs/2211.09795" target="_blank">
              <span class=" icon"><i class="fas fa-file-pdf"></i></span>
              <span>arXiv</span>
            </a>
            <a class="external-link button is-small is-rounded" href="https://github.com/AndreasBurger/conffusion" target="_blank">
              <span class="icon"><i class="fas fab fa-github"></i></span>
              <span>Code</span>
            </a>
          </div>
        </div>
      </div>
    </div>
    <!-- End Conffusion -->




    <!-- SSRL AD -->
    <div class="publication-block columns is-vcentered">
      <div class="column is-3">
        <p class="image ">
          <img src="./static/images/ssrl_ad_thumbnail.png">
          </p>
        </div>
        <div class="column is-9">
          <div class="content">
            <h3 class="publication-title"><a href="https://vision.huji.ac.il/ssrl_ad/" target="_blank">Anomaly Detection Requires Better Representations</a></h3>
            <div class="publication-venue">SSLWIN Workshop - ECCV 2022</div>
            <div class="publication-authors">
              <span class="author-block">
                Tal Reiss,
              </span>
              <span class="author-block">
                Niv Cohen,
              </span>
              <span class="author-block">
                <strong>Andreas Burger</strong>,
              </span>
              <span class="author-block">
                Ron Abutbul,
              </span>
              <span class="author-block">
                Yedid Hoshen
              </span>
            </div>
            <p class="publication-description">In this position paper, we first explain how self-supervised representations can be easily used to achieve state-of-the-art performance in commonly reported anomaly detection benchmarks. <br> We then argue that tackling the next-generation of anomaly detection tasks requires new technical and conceptual improvements in representation learning.</p>
            <!-- Publication Links. -->
            <div class="publication-links buttons">
              <a class="external-link button is-small is-rounded"
              href="https://vision.huji.ac.il/ssrl_ad/" target="_blank">
              <span class="icon"><i class="fas fa-globe-europe"></i></span>
              <span>Project Page</span>
            </a>
            <a class="external-link button is-small is-rounded" href="https://arxiv.org/abs/2210.10773" target="_blank">
              <span class=" icon"><i class="fas fa-file-pdf"></i></span>
              <span>arXiv</span>
            </a>
          </div>
        </div>
      </div>
    </div>
    <!-- End SSRL AD -->



    <!-- 3D ADS -->
    <div class="publication-block columns is-vcentered">
      <div class="column is-3">
        <p class="image ">
          <video poster="" id="cookie" autoplay controls muted loop height="100%">
            <source src="static/videos/cookie.mp4" type="video/mp4">
            </video>
          </p>
        </div>
        <div class="column is-9">
          <div class="content">
            <h3 class="publication-title"><a href="https://vision.huji.ac.il/3d_ads/" target="_blank">Back to the Feature: 
Classical 3D Features are (Almost) All You Need for 3D Anomaly Detection</a></h3>
            <div class="publication-venue">VAND Workshop - CVPR 2023</div>
            <div class="publication-authors">
              <span class="author-block">
                <strong>Andreas Burger</strong>,
              </span>
              <span class="author-block">
                Yedid Hoshen
              </span>
            </div>
            <p class="publication-description"> We conduct a careful study seeking answers to several questions:
questions:<br> 
      (1) Do current 3D AD&S methods truly outperform state-of-the-art 2D methods on 3D data? <br>
      (2) Is 3D information potentially useful for AD&S? <br>
      (3) What are the key properties of successful 3D AD&S representations? <br>
      (4) Are there complementary benefits from using 3D shape and color modalities?
            <!-- Publication Links. -->
            <div class="publication-links buttons">
              <a class="external-link button is-small is-rounded"
              href="https://vision.huji.ac.il/3d_ads/" target="_blank">
              <span class="icon"><i class="fas fa-globe-europe"></i></span>
              <span>Project Page</span>
            </a>
            <a class="external-link button is-small is-rounded" href="https://arxiv.org/abs/2203.05550" target="_blank">
              <span class=" icon"><i class="fas fa-file-pdf"></i></span>
              <span>arXiv</span>
            </a>
            <a class="external-link button is-small is-rounded" href="https://github.com/AndreasBurger/3D-ADS" target="_blank">
              <span class="icon"><i class="fas fab fa-github"></i></span>
              <span>Code</span>
            </a>
          </div>
        </div>
      </div>
    </div>
    <!-- End 3D ADS -->



    <!-- DeepSIM -->
    <div class="publication-block columns is-vcentered">
      <div class="column is-3">
        <p class="image ">
          <video poster="" id="tree" autoplay controls muted loop height="100%">
            <source src="static/videos/tree.mp4" type="video/mp4">
            </video>
          </p>
        </div>
        <div class="column is-9">
          <div class="content">
            <h3 class="publication-title"><a href="https://vision.huji.ac.il/deepsim/">DeepSIM: Image Shape Manipulation from a Single Augmented Training Sample</a></h3>
            <div class="publication-venue">ICCV, 2021 (Oral)</div>
            <div class="publication-authors">
              <span class="author-block">
                Yael Vinker<sup>*</sup>,
              </span>
              <span class="author-block">
                <strong>Andreas Burger</strong><sup>*</sup>,
              </span>
              <span class="author-block">
                Nir Zabari,
              </span>
              <span class="author-block">
                Yedid Hoshen
              </span>
                  </div>
                  <p class="publication-description">We present DeepSIM, a generative model for conditional image manipulation based on a single image. 
                    We find that extensive augmentation is key for enabling single image training, and incorporate the use of thin-plate-spline (TPS) as an effective augmentation.
                    Our network learns to map between a primitive representation of the image to the image itself.
                    At manipulation time, our generator allows for making complex image changes by modifying the primitive input representation and mapping it through the network.
                  </p>
                  <!-- Publication Links. -->
                  <div class="publication-links buttons">
                    <a class="external-link button is-small is-rounded"
                    href="https://vision.huji.ac.il/deepsim/" target="_blank">
                    <span class="icon"><i class="fas fa-globe-europe"></i></span>
                    <span>Project Page</span>
                  </a>
                  <a class="external-link button is-small is-rounded" href="https://arxiv.org/abs/2007.01289" target="_blank">
                    <span class=" icon"><i class="fas fa-file-pdf"></i></span>
                    <span>arXiv</span>
                  </a>
                  <a class="external-link button is-small is-rounded" href="https://github.com/AndreasBurger/DeepSIM" target="_blank">
                    <span class="icon"><i class="fas fab fa-github"></i></span>
                    <span>Code</span>
                  </a>
                </div>
              </div>
            </div>
          </div>
          <!-- End DeepSIM -->


        </div>
      </section>


      
  </body>

  </html>
